{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e60b535",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c8e9d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-10T19:05:30.649255Z",
     "end_time": "2023-05-10T19:05:30.692677Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "data_path = str(Path(os.getcwd()).parent.absolute())+\"/data\"\n",
    "figures_path = str(Path(os.getcwd()).parent.absolute())+\"/reports/figures\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e1562",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216c4a3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-10T19:05:37.271550Z",
     "end_time": "2023-05-10T19:05:37.317200Z"
    }
   },
   "outputs": [],
   "source": [
    "factors = pd.read_csv(data_path+\"/interim/factors.csv\", index_col=0)\n",
    "portfolio_excess_returns = pd.read_csv(data_path+\"/interim/portfolio_excess_returns.csv\", index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a733f818",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ec608",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-10T19:05:39.681030Z",
     "end_time": "2023-05-10T19:05:39.687082Z"
    }
   },
   "outputs": [],
   "source": [
    "# Return X and y for lasso taking penalty for beta estimates into account\n",
    "def get_betas_estimates(factors, portfolio_excess_returns, penalty):\n",
    "    #Compute the covariances between factors and the excess returns\n",
    "    cov_h = np.cov(factors.transpose(),portfolio_excess_returns.transpose())[len(factors.columns):,:len(factors.columns)]\n",
    "    cov_h = pd.DataFrame(data = cov_h, columns = factors.columns , index = portfolio_excess_returns.columns)\n",
    "\n",
    "    #Compute the average monthly returns of the portfolios\n",
    "    average_portfolio_excess_returns = pd.DataFrame({'Average Excess Returns': portfolio_excess_returns.mean()})\n",
    "\n",
    "    X = cov_h * penalty\n",
    "    y = average_portfolio_excess_returns\n",
    "    return(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8298dee",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T17:56:27.404359Z",
     "end_time": "2023-05-05T18:05:22.164239Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "alpha_search = pd.DataFrame()\n",
    "\n",
    "alphas = np.logspace(0,-35,100,base=np.e)\n",
    "random_state = 1\n",
    "cv_repeats = 20\n",
    "folds = 10\n",
    "\n",
    "#Compute the covariances between factors and the excess returns\n",
    "cov_h = np.cov(factors.transpose(),portfolio_excess_returns.transpose())[len(factors.columns):,:len(factors.columns)]\n",
    "cov_h = pd.DataFrame(data = cov_h, columns = factors.columns , index = portfolio_excess_returns.columns)\n",
    "\n",
    "#Compute the beta estimates\n",
    "factors_betas = pd.DataFrame(columns = portfolio_excess_returns.columns)\n",
    "for factor in factors:\n",
    "    factors_betas.loc[factor] = cov_h.loc[:,factor]/np.var(factors[factor])\n",
    "factors_betas = factors_betas.transpose()\n",
    "\n",
    "#Compute the penalty for the lasso\n",
    "penalty  = (factors_betas*factors_betas).mean()\n",
    "penalty = penalty/penalty.mean() # normalize the level\n",
    "\n",
    "X, y = get_betas_estimates(factors, portfolio_excess_returns, penalty)\n",
    "\n",
    "\n",
    "for cv_repeat in range(0,cv_repeats):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=cv_repeat)\n",
    "    print('\\n------------ CV Repeat number:', cv_repeat+1)\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(factors)):\n",
    "        print('\\n------ Fold Number:',fold+1)\n",
    "\n",
    "    # Second-Pass LASSO Regression First selection ###################################################\n",
    "        X_test, y_test = get_betas_estimates(factors.iloc[test_index], portfolio_excess_returns.iloc[test_index], penalty)\n",
    "        X_train, y_train = get_betas_estimates(factors.iloc[train_index], portfolio_excess_returns.iloc[train_index], penalty)\n",
    "\n",
    "        for alpha in alphas:\n",
    "            model = Lasso(alpha=alpha, fit_intercept=True).fit(X_train,y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            alpha_search.loc[len(alpha_search),['alpha', 'MSE','R-Squared','# Coefs',\"fold\",\"repeat\"]] = [alpha,mean_squared_error(y_test, y_pred),r2_score(y_test,y_pred),np.count_nonzero(model.coef_), fold, cv_repeat]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate k-fold mean and se\n",
    "cv_repeats = 200\n",
    "alphas = np.logspace(0,-35,100,base=np.e)\n",
    "alpha_search_new = pd.DataFrame()\n",
    "for cv_repeat in range(0,cv_repeats):\n",
    "    for alpha in alphas:\n",
    "        alpha_search_new.loc[len(alpha_search_new),['alpha', 'MSE','R-Squared','# Coefs', 'repeat', 'SE']]= [\n",
    "            alpha,\n",
    "            alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'MSE'].mean(),\n",
    "            alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'R-Squared'].mean(),\n",
    "            alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), '# Coefs'].mean(),\n",
    "            cv_repeat,\n",
    "            np.std(alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'MSE']),\n",
    "        ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-10T19:06:15.521274Z",
     "end_time": "2023-05-10T19:07:38.264850Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Choose best alpha for each repetition\n",
    "alpha_sel = pd.DataFrame()\n",
    "for cv_repeat in range(0,cv_repeats):\n",
    "    alpha_search_temp = alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat) & (alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat), 'MSE'] == alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat), 'MSE'].min())]\n",
    "    if alpha_search_temp.iloc[0,0] == 1:\n",
    "        alpha_temp = alpha_search_temp.iloc[-1,0]\n",
    "    else:\n",
    "        alpha_temp = alpha_search_temp.iloc[0,0]\n",
    "    alpha_sel.loc[cv_repeat,'alpha'] = alpha_temp\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T20:36:33.387500Z",
     "end_time": "2023-05-08T20:36:33.908211Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "alpha_sel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T20:36:43.200572Z",
     "end_time": "2023-05-08T20:36:43.225729Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the best alpha ie: the (log) average of the best alphas of each repetition\n",
    "alpha_sel.loc[:,'log_alpha'] = alpha_sel.loc[:,'alpha'].apply(np.log)\n",
    "best_alpha = np.exp(alpha_sel.loc[:,'log_alpha'].mean())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-08T20:36:57.483738Z",
     "end_time": "2023-05-08T20:36:57.524421Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Lasso(alpha=best_alpha, fit_intercept=True).fit(X,y)\n",
    "\n",
    "sel1_results = list(pd.DataFrame(data=abs(model.coef_), index=X.columns).sort_values(by=[0],ascending=False).iloc[0:np.count_nonzero(model.coef_)].index)\n",
    "sel1_results = [best_alpha] + sel1_results\n",
    "\n",
    "pd.DataFrame(data=sel1_results)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T18:17:11.740653Z",
     "end_time": "2023-05-05T18:17:11.785724Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Second selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "factors = pd.read_csv(data_path+\"/interim/factors.csv\", index_col=0).loc['2007-02-28':,:]\n",
    "portfolio_excess_returns = pd.read_csv(data_path+\"/interim/portfolio_excess_returns.csv\", index_col=0).loc['2007-02-28':'2017-10-31',:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T18:31:38.603070Z",
     "end_time": "2023-05-05T18:31:38.666494Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "green_factors = pd.read_csv(\"green_factors.csv\", index_col=0).loc['2007-02-28':'2017-10-31',:]\n",
    "green_factors = green_factors.loc[:,['ESG_ADJ', 'E_V_IND_W', 'S_V_IND_W', 'G_V_IND_W', 'CO2_V_IND_W']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T18:39:46.252718Z",
     "end_time": "2023-05-05T18:39:46.296502Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "alpha_search = pd.DataFrame()\n",
    "\n",
    "alphas = np.logspace(0,-35,100,base=np.e)\n",
    "random_state = 1\n",
    "cv_repeats = 20\n",
    "folds = 10\n",
    "\n",
    "\n",
    "for green_characteristic in green_factors.columns:\n",
    "    green_factor = pd.DataFrame(data = green_factors.loc[:, green_characteristic])\n",
    "\n",
    "    #Compute the covariances between factors and the excess returns\n",
    "    cov_h = np.cov(factors.transpose(),portfolio_excess_returns.transpose())[len(factors.columns):,:len(factors.columns)]\n",
    "    cov_h = pd.DataFrame(data = cov_h, columns = factors.columns , index = portfolio_excess_returns.columns)\n",
    "\n",
    "    #Compute the beta estimates\n",
    "    factors_betas = pd.DataFrame(columns = portfolio_excess_returns.columns)\n",
    "    for factor in factors:\n",
    "        factors_betas.loc[factor] = cov_h.loc[:,factor]/np.var(factors[factor])\n",
    "    factors_betas = factors_betas.transpose()\n",
    "\n",
    "    #Compute the penalty for the lasso\n",
    "    penalty  = (factors_betas*factors_betas).mean()\n",
    "    penalty = penalty/penalty.mean() # normalize the level\n",
    "\n",
    "    #Compute the covariances between green_factor and the excess returns\n",
    "    green_cov_h = np.cov(green_factor.transpose(),portfolio_excess_returns.transpose())[len(green_factor.columns):,:len(green_factor.columns)]\n",
    "    green_cov_h = pd.DataFrame(data = green_cov_h, columns = green_factor.columns , index = portfolio_excess_returns.columns)\n",
    "\n",
    "\n",
    "    #Compute the green_factor beta estimates\n",
    "    green_factor_betas = pd.DataFrame(columns = portfolio_excess_returns.columns)\n",
    "    for factor in green_factor:\n",
    "        green_factor_betas.loc[factor] = green_cov_h.loc[:,factor]/np.var(green_factor[factor])\n",
    "    green_factor_betas = green_factor_betas.transpose()\n",
    "\n",
    "\n",
    "    for cv_repeat in range(0,cv_repeats):\n",
    "        kf = KFold(n_splits=folds, shuffle=True, random_state=cv_repeat)\n",
    "        print('\\n------------ CV Repeat number:', cv_repeat+1)\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(factors)):\n",
    "            print('\\n------ Fold Number:',fold+1)\n",
    "\n",
    "            # Second-Pass LASSO Regression First selection ###################################################\n",
    "            X_test = get_betas_estimates(factors.iloc[test_index], portfolio_excess_returns.iloc[test_index], penalty)[0]\n",
    "            y_test = get_betas_estimates(green_factor.iloc[test_index], portfolio_excess_returns.iloc[test_index], 1)[0]\n",
    "\n",
    "            X_train = get_betas_estimates(factors.iloc[train_index], portfolio_excess_returns.iloc[train_index], penalty)[0]\n",
    "            y_train = get_betas_estimates(green_factor.iloc[train_index], portfolio_excess_returns.iloc[train_index], 1)[0]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                model = Lasso(alpha=alpha, fit_intercept=True, tol=0.000000001).fit(X_train,y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                alpha_search.loc[len(alpha_search),['alpha', 'MSE','R-Squared','# Coefs',\"fold\",\"repeat\"]] = [alpha,mean_squared_error(y_test, y_pred),r2_score(y_test,y_pred),np.count_nonzero(model.coef_), fold, cv_repeat]\n",
    "\n",
    "\n",
    "    # Calculate k-fold mean and se\n",
    "    alphas = np.logspace(0,-35,100,base=np.e)\n",
    "    alpha_search_new = pd.DataFrame()\n",
    "    for cv_repeat in range(0,cv_repeats):\n",
    "        for alpha in alphas:\n",
    "            alpha_search_new.loc[len(alpha_search_new),['alpha', 'MSE','R-Squared','# Coefs', 'repeat', 'SE']]= [\n",
    "                alpha,\n",
    "                alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'MSE'].mean(),\n",
    "                alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'R-Squared'].mean(),\n",
    "                alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), '# Coefs'].mean(),\n",
    "                cv_repeat,\n",
    "                np.std(alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'MSE']),\n",
    "            ]\n",
    "\n",
    "    # Choose best alpha for each repetition\n",
    "    alpha_sel = pd.DataFrame()\n",
    "    for cv_repeat in range(0,cv_repeats):\n",
    "        alpha_search_temp = alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat) & (alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat), 'MSE'] == alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat), 'MSE'].min())]\n",
    "        if alpha_search_temp.iloc[0,0] == 1:\n",
    "            alpha_temp = alpha_search_temp.iloc[-1,0]\n",
    "        else:\n",
    "            alpha_temp = alpha_search_temp.iloc[0,0]\n",
    "        alpha_sel.loc[cv_repeat,'alpha'] = alpha_temp\n",
    "\n",
    "    alpha_sel.loc[:,'log_alpha'] = alpha_sel.loc[:,'alpha'].apply(np.log)\n",
    "    best_alpha = np.exp(alpha_sel.loc[:,'log_alpha'].mean())\n",
    "\n",
    "    X = get_betas_estimates(factors, portfolio_excess_returns, penalty)[0]\n",
    "    y = get_betas_estimates(green_factor, portfolio_excess_returns, 1)[0]\n",
    "    model = Lasso(alpha=best_alpha, fit_intercept=True).fit(X,y)\n",
    "\n",
    "    sel2_results = list(pd.DataFrame(data=abs(model.coef_), index=X.columns).sort_values(by=[0],ascending=False).iloc[0:np.count_nonzero(model.coef_)].index)\n",
    "    sel2_results = [best_alpha] + sel2_results\n",
    "\n",
    "\n",
    "    pd.DataFrame(data=sel2_results).transpose().to_csv(data_path + '/processed/'+green_characteristic+'_sel2_results.csv')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T19:28:29.334227Z",
     "end_time": "2023-05-05T19:50:00.867685Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selection for inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "factors = pd.read_csv(data_path+\"/interim/factors.csv\", index_col=0).loc['2007-02-28':,:]\n",
    "portfolio_excess_returns = pd.read_csv(data_path+\"/interim/portfolio_excess_returns.csv\", index_col=0).loc['2007-02-28':'2017-10-31',:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T20:45:56.734508Z",
     "end_time": "2023-05-05T20:45:56.817157Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "green_factors = pd.read_csv(\"green_factors.csv\", index_col=0).loc['2007-02-28':'2017-10-31',:]\n",
    "green_factors = green_factors.loc[:,['ESG_ADJ', 'E_V_IND_W', 'S_V_IND_W', 'G_V_IND_W', 'CO2_V_IND_W']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T20:45:57.560667Z",
     "end_time": "2023-05-05T20:45:57.608029Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "alpha_search = pd.DataFrame()\n",
    "\n",
    "alphas = np.logspace(0,-35,100,base=np.e)\n",
    "random_state = 1\n",
    "cv_repeats = 20\n",
    "folds = 10\n",
    "\n",
    "\n",
    "for green_characteristic in green_factors.columns:\n",
    "    green_factor = pd.DataFrame(data = green_factors.loc[:, green_characteristic])\n",
    "\n",
    "\n",
    "    for cv_repeat in range(0,cv_repeats):\n",
    "        kf = KFold(n_splits=folds, shuffle=True, random_state=cv_repeat)\n",
    "        print('\\n------------ CV Repeat number:', cv_repeat+1)\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(factors)):\n",
    "            print('\\n------ Fold Number:',fold+1)\n",
    "\n",
    "            # Second-Pass LASSO Regression First selection ###################################################\n",
    "            X_test = factors.iloc[test_index]\n",
    "            # Standardize X\n",
    "            X_test = X_test / np.sqrt(len(X_test.index))\n",
    "            y_test = green_factor.iloc[test_index]\n",
    "\n",
    "            X_train = factors.iloc[train_index]\n",
    "            # Standardize X\n",
    "            X_train = X_train / np.sqrt(len(X_train.index))\n",
    "            y_train = green_factor.iloc[train_index]\n",
    "\n",
    "            for alpha in alphas:\n",
    "                model = Lasso(alpha=alpha, fit_intercept=False).fit(X_train,y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                alpha_search.loc[len(alpha_search),['alpha', 'MSE','R-Squared','# Coefs',\"fold\",\"repeat\"]] = [alpha,mean_squared_error(y_test, y_pred),r2_score(y_test,y_pred),np.count_nonzero(model.coef_), fold, cv_repeat]\n",
    "\n",
    "    # Calculate k-fold mean and se\n",
    "    alphas = np.logspace(0,-35,100,base=np.e)\n",
    "    alpha_search_new = pd.DataFrame()\n",
    "    for cv_repeat in range(0,cv_repeats):\n",
    "        for alpha in alphas:\n",
    "            alpha_search_new.loc[len(alpha_search_new),['alpha', 'MSE','R-Squared','# Coefs', 'repeat', 'SE']]= [\n",
    "                alpha,\n",
    "                alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'MSE'].mean(),\n",
    "                alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'R-Squared'].mean(),\n",
    "                alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), '# Coefs'].mean(),\n",
    "                cv_repeat,\n",
    "                np.std(alpha_search.loc[(alpha_search['alpha'] == alpha) & (alpha_search['repeat'] == cv_repeat), 'MSE']),\n",
    "            ]\n",
    "\n",
    "    # Choose best alpha for each repetition\n",
    "    alpha_sel = pd.DataFrame()\n",
    "    for cv_repeat in range(0,cv_repeats):\n",
    "        alpha_search_temp = alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat) & (alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat), 'MSE'] == alpha_search_new.loc[(alpha_search_new['repeat'] == cv_repeat), 'MSE'].min())]\n",
    "        if alpha_search_temp.iloc[0,0] == 1:\n",
    "            alpha_temp = alpha_search_temp.iloc[-1,0]\n",
    "        else:\n",
    "            alpha_temp = alpha_search_temp.iloc[0,0]\n",
    "        alpha_sel.loc[cv_repeat,'alpha'] = alpha_temp\n",
    "\n",
    "    # Select the best alpha ie: the (log) average of the best alphas of each repetition\n",
    "    alpha_sel.loc[:,'log_alpha'] = alpha_sel.loc[:,'alpha'].apply(np.log)\n",
    "    best_alpha = np.exp(alpha_sel.loc[:,'log_alpha'].mean())\n",
    "\n",
    "    X = factors\n",
    "    # Standardize X\n",
    "    X = X / np.sqrt(len(X.index))\n",
    "    y = green_factor\n",
    "    model = Lasso(alpha=best_alpha, fit_intercept=True).fit(X,y)\n",
    "\n",
    "    sel3_results = list(pd.DataFrame(data=abs(model.coef_), index=X.columns).sort_values(by=[0],ascending=False).iloc[0:np.count_nonzero(model.coef_)].index)\n",
    "    sel3_results = [best_alpha] + sel3_results\n",
    "\n",
    "    pd.DataFrame(data=sel3_results).transpose().to_csv(data_path + '/processed/'+green_characteristic+'_sel3_results.csv')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-05T21:27:47.648437Z",
     "end_time": "2023-05-05T21:45:40.828757Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 15:21:40) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "456eb8b4f0119b9c65dd9f66277c44667564c5e83ce409be82b9d4536b9aba1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
